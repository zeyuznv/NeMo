{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "import numpy as np\n",
    "from IPython.display import Audio, display\n",
    "import librosa\n",
    "import os\n",
    "import wget\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "import librosa\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use CTC labels for voice activity detection. To detect speech and non-speech segments in the audio, we use blank and space labels in the CTC outputs. Consecutive labels with spaces or blanks longer than a threshold are considered non-speech segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = os.getcwd()\n",
    "data_dir = os.path.join(ROOT,'data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# AUDIO_LIST_FILE = \"/mnt/data/amicorpus_lapel/lapel_files/amicorpus_test_wav.scp\"\n",
    "AUDIO_LIST_FILE = \"/home/fjia/data/modified_callhome/audio_evallist.txt\"\n",
    "\n",
    "AUDIO_LIST = []   \n",
    "with open(AUDIO_LIST_FILE, \"r\") as fp:\n",
    "    lines = fp.readlines()\n",
    "    for line in lines:\n",
    "        AUDIO_LIST.append(line.strip())\n",
    "                \n",
    "print(len(AUDIO_LIST))\n",
    "AUDIO_FILENAME = AUDIO_LIST[0]\n",
    "\n",
    "# audio, sample_rate = librosa.load(AUDIO_FILENAME)\n",
    "# ipd.Audio(audio, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "asr_model = nemo_asr.models.EncDecCTCModelBPE.from_pretrained(model_name='stt_en_citrinet_1024_gamma_0_25', strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = asr_model.decoder.vocabulary\n",
    "vocab.append(\"blank\") # bad practice actually need fix\n",
    "print(len(vocab))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = AUDIO_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax implementation in NumPy\n",
    "def softmax(logits):\n",
    "    e = np.exp(logits - np.max(logits))\n",
    "    return e / e.sum(axis=-1).reshape([logits.shape[0], 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_rttm(rttm_output_folder, AUDIO_FILENAME, non_speech, probs):\n",
    "\n",
    "    frame_offset=offset/time_stride\n",
    "    speech_labels=[]\n",
    "    uniq_id = os.path.basename(AUDIO_FILENAME).split('.wav')[0]\n",
    "    with open(os.path.join(rttm_output_folder, uniq_id+'.rttm'),'w') as f:\n",
    "        for idx in range(len(non_speech)-1):\n",
    "            start = (non_speech[idx][1]+frame_offset)*time_stride\n",
    "            end = (non_speech[idx+1][0]+frame_offset)*time_stride\n",
    "            \n",
    "            f.write(\"SPEAKER {} 1 {:.3f} {:.3f} <NA> <NA> speech <NA>\\n\".format(uniq_id,start,end-start))\n",
    "            speech_labels.append(\"{:.3f} {:.3f} speech\".format(start,end))\n",
    "        if non_speech[-1][1] < len(probs):\n",
    "            start = (non_speech[-1][1]+frame_offset)*time_stride\n",
    "            end = (len(probs)+frame_offset)*time_stride\n",
    "            f.write(\"SPEAKER {} 1 {:.3f} {:.3f} <NA> <NA> speech <NA>\\n\".format(uniq_id,start,end-start))\n",
    "            speech_labels.append(\"{:.3f} {:.3f} speech\".format(start,end))\n",
    "            \n",
    "    return speech_labels"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "â–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(probs, vocab, threshold):\n",
    "    # No space for citrinet just blanks\n",
    "    \n",
    "    blanks = []\n",
    "    state = ''\n",
    "    idx_state = 0\n",
    "    # TODO change all 1024 to dim\n",
    "    \n",
    "    # blank\n",
    "    if np.argmax(probs[0]) == 1024: #28 hard-coded bad!!\n",
    "        state = 'blank'\n",
    "\n",
    "    for idx in range(1, probs.shape[0]):\n",
    "        current_char_idx = int(np.argmax(probs[idx]))\n",
    "        if state == 'blank' and current_char_idx != 1024: #28\n",
    "            # previous is blank but current is not blank\n",
    "            blanks.append([idx_state, idx-1])\n",
    "            state = ''\n",
    "            \n",
    "        if state == '': # previous is not blank\n",
    "            if current_char_idx == 1024: #28\n",
    "                state = 'blank'\n",
    "                idx_state = idx\n",
    "\n",
    "    if state == 'blank':\n",
    "        blanks.append([idx_state, len(probs)-1])\n",
    "    \n",
    "    non_speech=list(filter(lambda x:x[1]-x[0]>threshold, blanks)) \n",
    "    print(len(blanks), len(non_speech))\n",
    "    return  non_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one_session(logits, threshold):\n",
    "    probs = softmax(logits)\n",
    "    non_speech = extract(probs, vocab, threshold)\n",
    "    return non_speech, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_logits = asr_model.transcribe(files, logprobs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 0.00 #-0.18  Confirmed with Patrice\n",
    "time_stride = 0.08 #0.02 Checked and Confirmed with Som\n",
    "\n",
    "threshold = 12 #minimun width to consider non-speech activity \n",
    "\n",
    "rttm_output_folder = \"/home/fjia/code/NeMo-fei/tutorials/speaker_recognition/\" + str(threshold) + \"_eval_citri_ch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(rttm_output_folder):\n",
    "    os.mkdir(rttm_output_folder)\n",
    "    \n",
    "for i in range(len(all_logits)):\n",
    "    logits = all_logits[i]\n",
    "    non_speech, probs = process_one_session(logits, threshold)\n",
    "    AUDIO_FILENAME = files[i]\n",
    "    speech_labels =  write_to_rttm(rttm_output_folder, AUDIO_FILENAME, non_speech, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths2rttm_files = glob.glob(rttm_output_folder + \"/*.rttm\")\n",
    "paths2rttm_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.asr.parts.utils.speaker_utils import write_rttm2manifest\n",
    "output_dir = os.path.join(ROOT, 'citrinet_vad_ch')\n",
    "os.makedirs(output_dir,exist_ok=True)\n",
    "oracle_manifest = os.path.join(output_dir, str(threshold)+'eval_manifest.json')\n",
    "write_rttm2manifest(paths2audio_files=files,\n",
    "                    paths2rttm_files=paths2rttm_files,\n",
    "                    manifest_file=oracle_manifest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat {output_dir}/2eval_manifest.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
